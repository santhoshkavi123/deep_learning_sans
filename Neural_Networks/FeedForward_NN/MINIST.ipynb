{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Handwritten Image Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The MNIST data set consists of 70,000 images of handwritten digits. It consists of digits from 0 to 9, and we are required to classify the class to which the image belongs. The images in the MNIST data set are 28X28 pixels, and the input layer has 784 neurons (each neuron takes 1 pixel as the input). The output layer has 10 neurons, with each giving the probability of the input image belonging to any of the 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Data Dealing packages \n",
    "import pickle # data serialization library\n",
    "import gzip # compression library\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "\n",
    "# decorator packages\n",
    "from sklearn.base import (TransformerMixin, \n",
    "                          BaseEstimator)\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some new libraries:\n",
    "1. pickle : Python Object serialisation library which converts python object into byte streams and vice versa\n",
    "2. gzip : Used to compress and decompress files with .gz extension\n",
    "3. h5py : This allows us to store and manipulate large numerical datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/kavisanthoshkumar/Library/CloudStorage/OneDrive-IllinoisInstituteofTechnology/MachineLearning_CodingStuff/deep_learning_sans/Neural_Networks/FeedForward_NN'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MNIST Dataset\n",
    "* The MNIST dataset which is divided into training, validation and test dataset\n",
    "* The User-defined function helps us to unpack the file and extracts the training, validation and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # Open gz file which is serialized object\n",
    "    f = gzip.open('/Users/kavisanthoshkumar/Library/CloudStorage/OneDrive-IllinoisInstituteofTechnology/MachineLearning_CodingStuff/mnist.pkl.gz', 'rb')\n",
    "    f.seek(0)\n",
    "\n",
    "    # Using pickle we deserialized the object\n",
    "    training_data, validation_data, test_data = pickle.load(\n",
    "        f,\n",
    "        encoding = 'latin1'\n",
    "    )\n",
    "\n",
    "    # Close the file\n",
    "    f.close()\n",
    "\n",
    "    return training_data, validation_data, test_data\n",
    "\n",
    "\n",
    "training_data, validation_data, test_data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the training feature dataset: (50000, 784)\n",
      " The feature dataset is: \n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "\n",
      " Shape of the training label dataset: (50000,)\n",
      "The label dataset is: \n",
      " [5 0 4 ... 8 4 8]\n",
      "Length of Unique Classes: 10\n",
      "\n",
      " The number of points in a single input is: 784\n"
     ]
    }
   ],
   "source": [
    "# feature and target dataset\n",
    "print(f\"Shape of the training feature dataset: {training_data[0].shape}\")\n",
    "print(f\" The feature dataset is: \\n {training_data[0]}\")\n",
    "\n",
    "print(f\"\\n\\n Shape of the training label dataset: {training_data[1].shape}\")\n",
    "print(f\"The label dataset is: \\n {training_data[1]}\")\n",
    "print(f\"Length of Unique Classes: {len(np.unique(training_data[1]))}\")\n",
    "\n",
    "# Number of datapoints in each input \n",
    "print(f\"\\n The number of points in a single input is: {len(training_data[0][1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Splitting the Wrapped dataset into feature and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unwrap Training dataset\n",
    "X_train = training_data[0]\n",
    "y_train = training_data[1]\n",
    "\n",
    "# Unwrap Validation dataset\n",
    "X_valid = validation_data[0]\n",
    "y_valid = validation_data[1]\n",
    "\n",
    "# Unwrap test dataset\n",
    "X_test = test_data[0]\n",
    "y_test = test_data[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Perform One-Hot Encoding on label dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(label_array: np.array) -> np.array:\n",
    "\n",
    "    # Create a zero matrix of shape = (length of label matrix, max label)\n",
    "    zero_arr = np.zeros((label_array.shape[0],\n",
    "              label_array.max() + 1))    \n",
    "    \n",
    "    # Update the zero_array \n",
    "    zero_arr[np.arange(zero_arr.shape[0]), label_array] = 1.0\n",
    "\n",
    "    return zero_arr\n",
    "\n",
    "\n",
    "y_train_encoded = one_hot_encoding(y_train)\n",
    "y_valid_encoded = one_hot_encoding(y_valid)\n",
    "y_test_encoded = one_hot_encoding(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set(X_train) shape: (50000, 784)\n",
      "training set(y_train) shape: (50000,)\n",
      "\n",
      "Validation set(X_valid) shape: (10000, 784)\n",
      "Validation set(y_valid) shape: (10000,)\n",
      "\n",
      "test set(X_test) shape: (10000, 784)\n",
      "test set(y_test) shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"training set(X_train) shape: {X_train.shape}\")\n",
    "print(f\"training set(y_train) shape: {y_train.shape}\")\n",
    "\n",
    "print(f\"\\nValidation set(X_valid) shape: {X_valid.shape}\")\n",
    "print(f\"Validation set(y_valid) shape: {y_valid.shape}\")\n",
    "\n",
    "print(f\"\\ntest set(X_test) shape: {X_test.shape}\")\n",
    "print(f\"test set(y_test) shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
